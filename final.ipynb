{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6403011,"sourceType":"datasetVersion","datasetId":3691903},{"sourceId":6403135,"sourceType":"datasetVersion","datasetId":3691990},{"sourceId":6403152,"sourceType":"datasetVersion","datasetId":3692003}],"dockerImageVersionId":30554,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install scipy --index-url=https://pypi.org/simple\n# !pip install scikit-image --index-url=https://pypi.org/simple","metadata":{"execution":{"iopub.status.busy":"2023-09-02T09:57:41.851380Z","iopub.execute_input":"2023-09-02T09:57:41.851723Z","iopub.status.idle":"2023-09-02T09:58:07.194169Z","shell.execute_reply.started":"2023-09-02T09:57:41.851693Z","shell.execute_reply":"2023-09-02T09:58:07.193031Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.23.5)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: numpy>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.23.5)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.11.2)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.0.9)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:16:31.255163Z","iopub.execute_input":"2023-09-04T06:16:31.255565Z","iopub.status.idle":"2023-09-04T06:16:31.465574Z","shell.execute_reply.started":"2023-09-04T06:16:31.255525Z","shell.execute_reply":"2023-09-04T06:16:31.464396Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.models as models\n\n# from skimage.feature import graycomatrix, graycoprops\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:19:54.154097Z","iopub.execute_input":"2023-09-04T06:19:54.154449Z","iopub.status.idle":"2023-09-04T06:19:54.160980Z","shell.execute_reply.started":"2023-09-04T06:19:54.154421Z","shell.execute_reply":"2023-09-04T06:19:54.159905Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"EPOCHS = 100","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:19:58.223042Z","iopub.execute_input":"2023-09-04T06:19:58.223697Z","iopub.status.idle":"2023-09-04T06:19:58.227773Z","shell.execute_reply.started":"2023-09-04T06:19:58.223651Z","shell.execute_reply":"2023-09-04T06:19:58.226772Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def Drawloss(loss_list, val_loss_list):\n    lens = len(loss_list)\n    fig = plt.figure(figsize=(8, 5))\n    fig.add_subplot(2,2,(1,4))\n    plt.style.use(\"ggplot\")\n\n    plt.plot(range(1, lens+1), loss_list, label=\"train_loss\")\n    plt.plot(range(1, lens+1), val_loss_list, label=\"val_loss\")\n\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"upper right\")\n\n    plt.show()\n    \ndef val_accuracy(model_path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if device.type == 'cuda':\n        model = torch.load(model_path)\n    else:\n        model = torch.load(model_path, map_location=torch.device('cpu'))\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(val_dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        accuracy = 100 * correct / total\n        print(f\"{model_path}, Val Accuracy: {accuracy:.2f}%\")\n        \ndef test_result(model_path, csv_filename):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if device.type == 'cuda':\n        model = torch.load(model_path)\n    else:\n        model = torch.load(model_path, map_location=torch.device('cpu'))\n    model.eval()\n    correct = 0\n    total = 0\n    predicted_list = []\n    with torch.no_grad():\n        for images, labels in tqdm(test_dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            predicted_list.append(predicted.item())\n            \n    test_df['Label'] = predicted_list\n    test_df.to_csv(f'{csv_filename}', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:00.205060Z","iopub.execute_input":"2023-09-04T06:20:00.205417Z","iopub.status.idle":"2023-09-04T06:20:00.220225Z","shell.execute_reply.started":"2023-09-04T06:20:00.205386Z","shell.execute_reply":"2023-09-04T06:20:00.219230Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 1. Data preprocess","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/csv-idlabel/train.csv')\ntest_df = pd.read_csv('/kaggle/input/csv-idlabel/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:03.912415Z","iopub.execute_input":"2023-09-04T06:20:03.912814Z","iopub.status.idle":"2023-09-04T06:20:03.934749Z","shell.execute_reply.started":"2023-09-04T06:20:03.912776Z","shell.execute_reply":"2023-09-04T06:20:03.933834Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, csv_path, images_folder, transform = False):\n        self.df = pd.read_csv(csv_path)\n        self.images_folder = images_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        filename = self.df.loc[index, \"ID\"]\n        label = self.df.loc[index, \"Label\"].item()\n        image = Image.open(os.path.join(self.images_folder, filename))\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:05.962241Z","iopub.execute_input":"2023-09-04T06:20:05.962709Z","iopub.status.idle":"2023-09-04T06:20:05.972225Z","shell.execute_reply.started":"2023-09-04T06:20:05.962673Z","shell.execute_reply":"2023-09-04T06:20:05.971195Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:08.144331Z","iopub.execute_input":"2023-09-04T06:20:08.144789Z","iopub.status.idle":"2023-09-04T06:20:08.155582Z","shell.execute_reply.started":"2023-09-04T06:20:08.144752Z","shell.execute_reply":"2023-09-04T06:20:08.154416Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"dataset = CustomDataset('/kaggle/input/csv-idlabel/train.csv','/kaggle/input/train-imgs/train_images', transform=transform)\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\ntest_dataset = CustomDataset('/kaggle/input/csv-idlabel/test.csv','/kaggle/input/test-imgs/test_images', transform=test_transform)\n\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:12.518069Z","iopub.execute_input":"2023-09-04T06:20:12.518431Z","iopub.status.idle":"2023-09-04T06:20:12.539809Z","shell.execute_reply.started":"2023-09-04T06:20:12.518400Z","shell.execute_reply":"2023-09-04T06:20:12.538909Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for images, labels in train_dataloader:\n    print(images.shape)\n    print(labels.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:14.193166Z","iopub.execute_input":"2023-09-04T06:20:14.193532Z","iopub.status.idle":"2023-09-04T06:20:14.549280Z","shell.execute_reply.started":"2023-09-04T06:20:14.193491Z","shell.execute_reply":"2023-09-04T06:20:14.548284Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([32, 1, 512, 512])\ntorch.Size([32])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class EarlyStopper:\n    def __init__(self, model_path, patience=20, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.model_path = model_path\n        self.counter = 0\n        self.min_val_loss = np.inf\n\n    def check(self, val_loss, model):\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.counter = 0\n            torch.save(model, self.model_path)\n        elif val_loss > (self.min_val_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False ","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:16.527274Z","iopub.execute_input":"2023-09-04T06:20:16.527692Z","iopub.status.idle":"2023-09-04T06:20:16.536901Z","shell.execute_reply.started":"2023-09-04T06:20:16.527656Z","shell.execute_reply":"2023-09-04T06:20:16.535771Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# 2. PSPNet with deeper Model","metadata":{}},{"cell_type":"code","source":"model_path = f'/kaggle/working/PSPNetDeeper_epoch{EPOCHS}_kaggle.pt'\npredict_csv_path = f'/kaggle/working/PSPNetDeeper_epoch{EPOCHS}_kaggle.csv'","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:19.323539Z","iopub.execute_input":"2023-09-04T06:20:19.323924Z","iopub.status.idle":"2023-09-04T06:20:19.328799Z","shell.execute_reply.started":"2023-09-04T06:20:19.323894Z","shell.execute_reply":"2023-09-04T06:20:19.327720Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class PSPNet(nn.Module):\n    def __init__(self, num_classes):\n        super(PSPNet, self).__init__()\n        \n        # Conv layers\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1), # output size (N, 16, 512, 512)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 16, 256, 256)\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1), # output size (N, 32, 256, 256)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 32, 128, 128)\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), # output size (N, 64, 128, 128)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 64, 64, 64)\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), # output size (N, 128, 64, 64)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 128, 32, 32)\n        )\n        # Spatial Pyramid Pooling layers\n        self.pool1 = nn.AdaptiveMaxPool2d((1, 1)) # output size (N, 64, 1, 1)\n        self.pool2 = nn.AdaptiveMaxPool2d((2, 2)) # output size (N, 64, 2, 2)\n        self.pool3 = nn.AdaptiveMaxPool2d((3, 3)) # output size (N, 64, 3, 3)\n        self.pool4 = nn.AdaptiveMaxPool2d((6, 6)) # output size (N, 64, 6, 6)\n        self.con1 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 1, 1)\n        self.con2 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 2, 2)\n        self.con3 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 3, 3)\n        self.con4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 6, 6)\n        # Upsampling layers\n        self.upsample1 = nn.Upsample(scale_factor=32/1, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample2 = nn.Upsample(scale_factor=32/2, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample3 = nn.Upsample(scale_factor=32/3, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample4 = nn.Upsample(scale_factor=32/6, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        # Conv Classifier layers\n        self.classifier = nn.Sequential(\n            nn.Conv2d(in_channels=132, out_channels=64, kernel_size=3, stride=2, padding=1), # output size (N, 64, 16, 16)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 64, 8, 8)\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1), # output size (N, 32, 4, 4)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 32, 2, 2)\n            nn.Flatten(), # output size (N, 32 * 2* 2)\n            nn.Linear(32 * 2 * 2, 32), # output size (N, 32)\n            nn.ReLU(),\n            nn.Linear(32, num_classes), # output size (N, 6)\n        )\n        \n    def forward(self, x):\n        # CNN layers\n        x = self.features(x)\n        \n        # Spatial Pyramid Pooling\n        x1 = self.pool1(x)\n        x1 = self.con1(x1) \n        x2 = self.pool2(x)\n        x2 = self.con2(x2)\n        x3 = self.pool3(x)\n        x3 = self.con3(x3)\n        x4 = self.pool4(x)\n        x4 = self.con4(x4)\n        \n        # Upsampling\n        x1 = self.upsample1(x1)\n        x2 = self.upsample2(x2)\n        x3 = self.upsample3(x3)\n        x4 = self.upsample4(x4)\n        \n        # Concatenate the pooled features\n        x = torch.cat((x1, x2, x3, x4, x), dim=1) # output size (N, 132, 32, 32)\n        \n        # Classifier\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:21.757149Z","iopub.execute_input":"2023-09-04T06:20:21.757525Z","iopub.status.idle":"2023-09-04T06:20:21.775635Z","shell.execute_reply.started":"2023-09-04T06:20:21.757476Z","shell.execute_reply":"2023-09-04T06:20:21.774574Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = PSPNet(num_classes=6).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nloss_list = []\nval_loss_list = []\nearly_stopper = EarlyStopper(model_path = model_path)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    for images, labels in train_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    loss_list.append(loss.item())\n    \n    model.eval()\n    with torch.no_grad():\n        tmp_loss_list = []\n        for images, labels in val_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            val_loss = criterion(outputs, labels)\n            tmp_loss_list.append(val_loss.item())\n        avg_val_loss = sum(tmp_loss_list)/len(tmp_loss_list)\n        val_loss_list.append(avg_val_loss)\n        if early_stopper.check(avg_val_loss, model):\n            print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.4f}, Val Loss:{avg_val_loss:.4f},\\nEarly stop in {epoch+1}!!')\n            break\n            \n    if (epoch+1) % 1 == 0 or epoch == 0:\n        print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.4f}, Val Loss:{avg_val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:20:25.154558Z","iopub.execute_input":"2023-09-04T06:20:25.154998Z","iopub.status.idle":"2023-09-04T06:37:32.859416Z","shell.execute_reply.started":"2023-09-04T06:20:25.154968Z","shell.execute_reply":"2023-09-04T06:37:32.857575Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [1/100], Train Loss: 1.7022, Val Loss:1.6562\nEpoch [2/100], Train Loss: 1.4983, Val Loss:1.6216\nEpoch [3/100], Train Loss: 0.9071, Val Loss:1.1902\nEpoch [4/100], Train Loss: 1.1987, Val Loss:1.0668\nEpoch [5/100], Train Loss: 1.4066, Val Loss:0.7340\nEpoch [6/100], Train Loss: 0.3545, Val Loss:0.4360\nEpoch [7/100], Train Loss: 0.7745, Val Loss:0.3941\nEpoch [8/100], Train Loss: 0.0501, Val Loss:0.3313\nEpoch [9/100], Train Loss: 0.5201, Val Loss:0.6773\nEpoch [10/100], Train Loss: 0.0021, Val Loss:0.3413\nEpoch [11/100], Train Loss: 0.2246, Val Loss:0.3155\nEpoch [12/100], Train Loss: 0.0195, Val Loss:0.3296\nEpoch [13/100], Train Loss: 0.0480, Val Loss:0.2751\nEpoch [14/100], Train Loss: 0.3610, Val Loss:0.2136\nEpoch [15/100], Train Loss: 0.3463, Val Loss:0.1827\nEpoch [16/100], Train Loss: 0.0494, Val Loss:0.2001\nEpoch [17/100], Train Loss: 0.1086, Val Loss:0.1781\nEpoch [18/100], Train Loss: 0.0511, Val Loss:0.1557\nEpoch [19/100], Train Loss: 0.0233, Val Loss:0.7034\nEpoch [20/100], Train Loss: 0.0575, Val Loss:0.1779\nEpoch [21/100], Train Loss: 0.0000, Val Loss:0.1106\nEpoch [22/100], Train Loss: 0.0055, Val Loss:0.1498\nEpoch [23/100], Train Loss: 0.0047, Val Loss:0.1381\nEpoch [24/100], Train Loss: 0.1248, Val Loss:0.1075\nEpoch [25/100], Train Loss: 0.0197, Val Loss:0.2201\nEpoch [26/100], Train Loss: 0.0206, Val Loss:0.0597\nEpoch [27/100], Train Loss: 0.0105, Val Loss:0.1326\nEpoch [28/100], Train Loss: 0.0013, Val Loss:0.0727\nEpoch [29/100], Train Loss: 0.0119, Val Loss:0.1611\nEpoch [30/100], Train Loss: 0.0054, Val Loss:0.0685\nEpoch [31/100], Train Loss: 0.0105, Val Loss:0.0915\nEpoch [32/100], Train Loss: 0.0019, Val Loss:0.1016\nEpoch [33/100], Train Loss: 0.0015, Val Loss:0.0690\nEpoch [34/100], Train Loss: 0.0001, Val Loss:0.0587\nEpoch [35/100], Train Loss: 0.0594, Val Loss:0.0843\nEpoch [36/100], Train Loss: 0.2465, Val Loss:0.0729\nEpoch [37/100], Train Loss: 0.0212, Val Loss:0.0771\nEpoch [38/100], Train Loss: 0.0044, Val Loss:0.0882\nEpoch [39/100], Train Loss: 0.0145, Val Loss:0.1330\nEpoch [40/100], Train Loss: 0.0102, Val Loss:0.0487\nEpoch [41/100], Train Loss: 0.0005, Val Loss:0.0417\nEpoch [42/100], Train Loss: 0.0015, Val Loss:0.0811\nEpoch [43/100], Train Loss: 0.0002, Val Loss:0.0493\nEpoch [44/100], Train Loss: 0.0018, Val Loss:0.1795\nEpoch [45/100], Train Loss: 0.0005, Val Loss:0.0831\nEpoch [46/100], Train Loss: 0.0065, Val Loss:0.3842\nEpoch [47/100], Train Loss: 0.0051, Val Loss:0.0436\nEpoch [48/100], Train Loss: 0.0000, Val Loss:0.0636\nEpoch [49/100], Train Loss: 0.0005, Val Loss:0.0340\nEpoch [50/100], Train Loss: 0.5233, Val Loss:0.0519\nEpoch [51/100], Train Loss: 0.0010, Val Loss:0.1255\nEpoch [52/100], Train Loss: 0.0023, Val Loss:0.1236\nEpoch [53/100], Train Loss: 0.0149, Val Loss:0.0401\nEpoch [54/100], Train Loss: 0.0392, Val Loss:0.1618\nEpoch [55/100], Train Loss: 0.0055, Val Loss:0.0466\nEpoch [56/100], Train Loss: 0.0147, Val Loss:0.0289\nEpoch [57/100], Train Loss: 0.0187, Val Loss:0.0698\nEpoch [58/100], Train Loss: 0.0010, Val Loss:0.0551\nEpoch [59/100], Train Loss: 0.0001, Val Loss:0.0667\nEpoch [60/100], Train Loss: 0.0029, Val Loss:0.0533\nEpoch [61/100], Train Loss: 0.0002, Val Loss:0.0572\nEpoch [62/100], Train Loss: 0.0102, Val Loss:0.1053\nEpoch [63/100], Train Loss: 0.0101, Val Loss:0.0535\nEpoch [64/100], Train Loss: 0.0065, Val Loss:0.0795\nEpoch [65/100], Train Loss: 0.0002, Val Loss:0.0836\nEpoch [66/100], Train Loss: 0.0004, Val Loss:0.1788\nEpoch [67/100], Train Loss: 0.0022, Val Loss:0.0985\nEpoch [68/100], Train Loss: 0.0003, Val Loss:0.0612\nEpoch [69/100], Train Loss: 0.0003, Val Loss:0.0472\nEpoch [70/100], Train Loss: 0.0086, Val Loss:0.0467\nEpoch [71/100], Train Loss: 0.0113, Val Loss:0.4213\nEpoch [72/100], Train Loss: 0.0046, Val Loss:0.1354\nEpoch [73/100], Train Loss: 0.0004, Val Loss:0.0701\nEpoch [74/100], Train Loss: 0.0001, Val Loss:0.0509\nEpoch [75/100], Train Loss: 0.0101, Val Loss:0.0665\nEpoch [76/100], Train Loss: 0.0017, Val Loss:0.0792,\nEarly stop in 76!!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"Drawloss(loss_list, val_loss_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T05:26:59.787657Z","iopub.status.idle":"2023-09-04T05:26:59.788517Z","shell.execute_reply.started":"2023-09-04T05:26:59.788249Z","shell.execute_reply":"2023-09-04T05:26:59.788273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.save(model, model_path)\nval_accuracy(model_path)\ntest_result(model_path, predict_csv_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T09:58:11.585692Z","iopub.status.idle":"2023-09-02T09:58:11.586478Z","shell.execute_reply.started":"2023-09-02T09:58:11.586234Z","shell.execute_reply":"2023-09-02T09:58:11.586257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2.5 PSPNet + GLCM features","metadata":{}},{"cell_type":"code","source":"model_path = f'/kaggle/working/PSPNetGLCM_epoch{EPOCHS}_kaggle.pt'\npredict_csv_path = f'/kaggle/working/PSPNetGLCM_epoch{EPOCHS}_kaggle.csv'","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:09:13.885910Z","iopub.execute_input":"2023-09-04T06:09:13.886264Z","iopub.status.idle":"2023-09-04T06:09:13.891166Z","shell.execute_reply.started":"2023-09-04T06:09:13.886236Z","shell.execute_reply":"2023-09-04T06:09:13.889809Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def GLCM_features(image):\n    image = np.array(image)\n    image = (image * 255).astype(np.uint8)\n    glcm_features = torch.empty(25, dtype=torch.float32)\n\n    #5 configuration for the grey-level co-occurrence matrix calculation\n    dists = [[1],[3],[5],[3],[3],[3]]\n    angles = [[0],[0],[0],[np.pi/4],[np.pi/2],[np.pi*3/4],]\n\n    for j ,(dist, angle) in enumerate(zip(dists, angles)):\n        GLCM = graycomatrix(image, dist, angle) \n        glcm_features[j*5] = torch.tensor(graycoprops(GLCM, 'energy')[0], dtype=torch.float32)\n        glcm_features[j*5 + 1] = torch.tensor(graycoprops(GLCM, 'correlation')[0] , dtype=torch.float32)   \n        glcm_features[j*5 + 2] = torch.tensor(graycoprops(GLCM, 'dissimilarity')[0], dtype=torch.float32)\n        glcm_features[j*5 + 3] = torch.tensor(graycoprops(GLCM, 'homogeneity')[0], dtype=torch.float32)\n        glcm_features[j*5 + 4] = torch.tensor(graycoprops(GLCM, 'contrast')[0], dtype=torch.float32)\n        \n    return glcm_features","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GLCMDataset(Dataset):\n    def __init__(self, csv_path, images_folder, transform = False):\n        self.df = pd.read_csv(csv_path)\n        self.images_folder = images_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        filename = self.df.loc[index, \"ID\"]\n        label = self.df.loc[index, \"Label\"].item()\n        image = Image.open(os.path.join(self.images_folder, filename))\n        glcm_feature = GLCM_features(image)\n        if self.transform:\n            image = self.transform(image)\n        return image, label, glcm_feature","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = GLCMDataset('/kaggle/input/csv-index/train.csv','/kaggle/input/train-image-aoi/train_images', transform=transform)\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\ntest_dataset = GLCMDataset('/kaggle/input/csv-index/test.csv','/kaggle/input/test-image-aoi/test_images', transform=test_transform)\n\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, labels, glcm_features in train_dataloader:\n    print(images.shape)\n    print(labels.shape)\n    print(glcm_features.shape)\n    break","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PSPNetGLCM(nn.Module):\n    def __init__(self, num_classes):\n        super(PSPNetGLCM, self).__init__()\n        \n        # Conv layers\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1), # output size (N, 16, 512, 512)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 16, 256, 256)\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1), # output size (N, 32, 256, 256)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 32, 128, 128)\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), # output size (N, 64, 128, 128)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 64, 64, 64)\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), # output size (N, 128, 64, 64)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 128, 32, 32)\n        )\n        # Spatial Pyramid Pooling layers\n        self.pool1 = nn.AdaptiveMaxPool2d((1, 1)) # output size (N, 128, 1, 1)\n        self.pool2 = nn.AdaptiveMaxPool2d((2, 2)) # output size (N, 128, 2, 2)\n        self.pool3 = nn.AdaptiveMaxPool2d((3, 3)) # output size (N, 128, 3, 3)\n        self.pool4 = nn.AdaptiveMaxPool2d((6, 6)) # output size (N, 128, 6, 6)\n        self.con1 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 1, 1)\n        self.con2 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 2, 2)\n        self.con3 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 3, 3)\n        self.con4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 6, 6)\n        # Upsampling layers\n        self.upsample1 = nn.Upsample(scale_factor=32/1, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample2 = nn.Upsample(scale_factor=32/2, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample3 = nn.Upsample(scale_factor=32/3, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample4 = nn.Upsample(scale_factor=32/6, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        \n        # Conv Classifier layers\n        self.nn_classifier = nn.Sequential(\n            nn.Conv2d(in_channels=132, out_channels=64, kernel_size=3, stride=2, padding=1), # output size (N, 64, 16, 16)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 64, 8, 8)\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1), # output size (N, 32, 4, 4)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 32, 2, 2)\n            nn.Flatten(), # output size (N, 32 * 2* 2)\n            nn.Linear(32 * 2 * 2, 24), # output size (N, 24)\n            nn.ReLU(),\n        )\n        self.glcm_classifier= nn.Sequential(\n            nn.Linear(25, 8), # output size (N, 8)\n            nn.ReLU(),\n        )\n        self.final_classifier = nn.Sequential(\n            nn.Linear(32, num_classes) # output size (N, num_classes=6)\n        )\n        \n    def forward(self, x_input, x_glcm):\n        # CNN layers\n        x = self.features(x_input)\n        \n        # Spatial Pyramid Pooling\n        x1 = self.pool1(x)\n        x1 = self.con1(x1) \n        x2 = self.pool2(x)\n        x2 = self.con2(x2)\n        x3 = self.pool3(x)\n        x3 = self.con3(x3)\n        x4 = self.pool4(x)\n        x4 = self.con4(x4)\n        \n        # Upsampling\n        x1 = self.upsample1(x1)\n        x2 = self.upsample2(x2)\n        x3 = self.upsample3(x3)\n        x4 = self.upsample4(x4)\n        \n        # Concatenate the pooled features\n        x = torch.cat((x1, x2, x3, x4, x), dim=1) # output size (N, 132, 32, 32)\n        \n        # Classifier\n        x = self.nn_classifier(x) # output size (N, 24)\n        \n        # Get GLCM features \n        x_glcm = self.glcm_classifier(x_glcm) # output size (N, 8)\n        \n        # Concatenate nn features and GLCM features\n        x = torch.cat((x, x_glcm), dim=1) # output size (N, 32, 32)\n        \n        # final classifier\n        x = self.final_classifier(x)\n        \n        return x","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = PSPNetGLCM(num_classes=6).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nloss_list = []\nval_loss_list = []\nearly_stopper = EarlyStopper(model_path = model_path)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    for images, labels, glcm_features in tqdm(train_dataloader):\n        images = images.to(device)\n        labels = labels.to(device)\n        glcm_features = glcm_features.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images, glcm_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    loss_list.append(loss.item())\n    \n    model.eval()\n    with torch.no_grad():\n        tmp_loss_list = []\n        for images, labels, glcm_features in val_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            glcm_features = glcm_features.to(device)\n            \n            outputs = model(images, glcm_features)\n            val_loss = criterion(outputs, labels)\n            tmp_loss_list.append(val_loss.item())\n        avg_val_loss = sum(tmp_loss_list)/len(tmp_loss_list)\n        val_loss_list.append(avg_val_loss)\n        if early_stopper.check(avg_val_loss, model):\n            print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.4f}, Val Loss:{avg_val_loss:.4f},\\nEarly stop in {epoch+1}!!')\n            break\n            \n    if (epoch+1) % 1 == 0 or epoch == 0:\n        print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.4f}, Val Loss:{avg_val_loss:.4f}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def val_accuracy(model_path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if device.type == 'cuda':\n        model = torch.load(model_path)\n    else:\n        model = torch.load(model_path, map_location=torch.device('cpu'))\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels, glcm_features in tqdm(val_dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            glcm_features = glcm_features.to(device)\n            \n            outputs = model(images, glcm_features)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        accuracy = 100 * correct / total\n        print(f\"{model_path}, Val Accuracy: {accuracy:.2f}%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_result(model_path, csv_filename):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if device.type == 'cuda':\n        model = torch.load(model_path)\n    else:\n        model = torch.load(model_path, map_location=torch.device('cpu'))\n    model.eval()\n    correct = 0\n    total = 0\n    predicted_list = []\n    with torch.no_grad():\n        for images, labels, glcm_features in tqdm(test_dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            glcm_features = glcm_features.to(device)\n            \n            outputs = model(images, glcm_features)\n            _, predicted = torch.max(outputs.data, 1)\n            predicted_list.append(predicted.item())\n            \n    test_df['Label'] = predicted_list\n    test_df.to_csv(f'{csv_filename}', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drawloss(loss_list, val_loss_list)\nval_accuracy(model_path)\ntest_result(model_path, predict_csv_path)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.","metadata":{}},{"cell_type":"code","source":"model_path = f'/kaggle/working/PSPNetDeeper_epoch{EPOCHS}_kaggle.pt'\npredict_csv_path = f'/kaggle/working/PSPNetDeeper_epoch{EPOCHS}_kaggle.csv'","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:51:52.057324Z","iopub.execute_input":"2023-09-04T06:51:52.057691Z","iopub.status.idle":"2023-09-04T06:51:52.062480Z","shell.execute_reply.started":"2023-09-04T06:51:52.057662Z","shell.execute_reply":"2023-09-04T06:51:52.061513Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"pip install mahotas","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:51:59.193315Z","iopub.execute_input":"2023-09-04T06:51:59.193687Z","iopub.status.idle":"2023-09-04T06:52:12.917458Z","shell.execute_reply.started":"2023-09-04T06:51:59.193656Z","shell.execute_reply":"2023-09-04T06:52:12.916260Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting mahotas\n  Downloading mahotas-1.4.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mahotas) (1.23.5)\nInstalling collected packages: mahotas\nSuccessfully installed mahotas-1.4.13\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import mahotas as mh\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:52:15.918373Z","iopub.execute_input":"2023-09-04T06:52:15.919451Z","iopub.status.idle":"2023-09-04T06:52:15.941653Z","shell.execute_reply.started":"2023-09-04T06:52:15.919406Z","shell.execute_reply":"2023-09-04T06:52:15.940639Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def Haralick_features(image):\n    features = mh.features.haralick(image).mean(axis=0)\n    features = torch.from_numpy(features).type(torch.float32)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:52:28.224705Z","iopub.execute_input":"2023-09-04T06:52:28.225123Z","iopub.status.idle":"2023-09-04T06:52:28.230370Z","shell.execute_reply.started":"2023-09-04T06:52:28.225093Z","shell.execute_reply":"2023-09-04T06:52:28.229393Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, csv_path, images_folder, transform = False):\n        self.df = pd.read_csv(csv_path)\n        self.images_folder = images_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        filename = self.df.loc[index, \"ID\"]\n        label = self.df.loc[index, \"Label\"].item()\n        image = cv2.imread(os.path.join(self.images_folder, filename))\n        features = Haralick_features(image)\n        image1 = Image.open(os.path.join(self.images_folder, filename))\n        if self.transform:\n            image1 = self.transform(image1)\n        return image1, label, features","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:52:30.974935Z","iopub.execute_input":"2023-09-04T06:52:30.975291Z","iopub.status.idle":"2023-09-04T06:52:30.983166Z","shell.execute_reply.started":"2023-09-04T06:52:30.975262Z","shell.execute_reply":"2023-09-04T06:52:30.982213Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"dataset = MyDataset('/kaggle/input/csv-idlabel/train.csv','/kaggle/input/train-imgs/train_images', transform=transform)\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\ntest_dataset = MyDataset('/kaggle/input/csv-idlabel/test.csv','/kaggle/input/test-imgs/test_images', transform=test_transform)\n\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:52:34.143942Z","iopub.execute_input":"2023-09-04T06:52:34.144317Z","iopub.status.idle":"2023-09-04T06:52:34.178111Z","shell.execute_reply.started":"2023-09-04T06:52:34.144286Z","shell.execute_reply":"2023-09-04T06:52:34.177164Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"for images, labels, features in train_dataloader:\n    print(images.shape)\n    print(labels.shape)\n    print(features.shape)\n\n    print(images.dtype)\n    print(features.dtype)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:52:37.024003Z","iopub.execute_input":"2023-09-04T06:52:37.024392Z","iopub.status.idle":"2023-09-04T06:52:46.120035Z","shell.execute_reply.started":"2023-09-04T06:52:37.024362Z","shell.execute_reply":"2023-09-04T06:52:46.116869Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([32, 1, 512, 512])\ntorch.Size([32])\ntorch.Size([32, 13])\ntorch.float32\ntorch.float32\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"class PSPNetGLCM(nn.Module):\n    def __init__(self, num_classes):\n        super(PSPNetGLCM, self).__init__()\n\n        # Conv layers\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1), # output size (N, 16, 512, 512)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 16, 256, 256)\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1), # output size (N, 32, 256, 256)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 32, 128, 128)\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), # output size (N, 64, 128, 128)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 64, 64, 64)\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), # output size (N, 128, 64, 64)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 128, 32, 32)\n        )\n        # Spatial Pyramid Pooling layers\n        self.pool1 = nn.AdaptiveMaxPool2d((1, 1)) # output size (N, 128, 1, 1)\n        self.pool2 = nn.AdaptiveMaxPool2d((2, 2)) # output size (N, 128, 2, 2)\n        self.pool3 = nn.AdaptiveMaxPool2d((3, 3)) # output size (N, 128, 3, 3)\n        self.pool4 = nn.AdaptiveMaxPool2d((6, 6)) # output size (N, 128, 6, 6)\n        self.con1 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 1, 1)\n        self.con2 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 2, 2)\n        self.con3 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 3, 3)\n        self.con4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0) # output size (N, 1, 6, 6)\n        # Upsampling layers\n        self.upsample1 = nn.Upsample(scale_factor=32/1, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample2 = nn.Upsample(scale_factor=32/2, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample3 = nn.Upsample(scale_factor=32/3, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n        self.upsample4 = nn.Upsample(scale_factor=32/6, mode='bilinear', align_corners=True) # output size (N, 1, 32, 32)\n\n        # Conv Classifier layers\n        self.nn_classifier = nn.Sequential(\n            nn.Conv2d(in_channels=132, out_channels=64, kernel_size=3, stride=2, padding=1), # output size (N, 64, 16, 16)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 64, 8, 8)\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1), # output size (N, 32, 4, 4)\n            nn.ReLU(),\n            nn.MaxPool2d(2), # output size (N, 32, 2, 2)\n            nn.Flatten(), # output size (N, 32 * 2* 2)\n            nn.Linear(32 * 2 * 2, 24), # output size (N, 24)\n            nn.ReLU(),\n        )\n        self.glcm_classifier= nn.Sequential(\n            nn.Linear(13, 8), # output size (N, 8)\n            nn.ReLU(),\n        )\n        self.final_classifier = nn.Sequential(\n            nn.Linear(32, num_classes) # output size (N, num_classes=6)\n        )\n\n    def forward(self, x_input, x_glcm):\n        # CNN layers\n        x = self.features(x_input)\n\n        # Spatial Pyramid Pooling\n        x1 = self.pool1(x)\n        x1 = self.con1(x1)\n        x2 = self.pool2(x)\n        x2 = self.con2(x2)\n        x3 = self.pool3(x)\n        x3 = self.con3(x3)\n        x4 = self.pool4(x)\n        x4 = self.con4(x4)\n\n        # Upsampling\n        x1 = self.upsample1(x1)\n        x2 = self.upsample2(x2)\n        x3 = self.upsample3(x3)\n        x4 = self.upsample4(x4)\n\n        # Concatenate the pooled features\n        x = torch.cat((x1, x2, x3, x4, x), dim=1) # output size (N, 132, 32, 32)\n\n        # Classifier\n        x = self.nn_classifier(x) # output size (N, 24)\n\n        # Get GLCM features\n        x_glcm = self.glcm_classifier(x_glcm) # output size (N, 8)\n\n        # Concatenate nn features and GLCM features\n        x = torch.cat((x, x_glcm), dim=1) # output size (N, 32, 32)\n\n        # final classifier\n        x = self.final_classifier(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:53:08.973534Z","iopub.execute_input":"2023-09-04T06:53:08.974067Z","iopub.status.idle":"2023-09-04T06:53:08.997383Z","shell.execute_reply.started":"2023-09-04T06:53:08.974032Z","shell.execute_reply":"2023-09-04T06:53:08.996228Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel = PSPNetGLCM(num_classes=6).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nloss_list = []\nval_loss_list = []\nearly_stopper = EarlyStopper(model_path = model_path)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    for images, labels, glcm_features in tqdm(train_dataloader):\n        images = images.to(device)\n        labels = labels.to(device)\n        glcm_features = glcm_features.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images, glcm_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    loss_list.append(loss.item())\n\n    model.eval()\n    with torch.no_grad():\n        tmp_loss_list = []\n        for images, labels, glcm_features in val_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            glcm_features = glcm_features.to(device)\n\n            outputs = model(images, glcm_features)\n            val_loss = criterion(outputs, labels)\n            tmp_loss_list.append(val_loss.item())\n        avg_val_loss = sum(tmp_loss_list)/len(tmp_loss_list)\n        val_loss_list.append(avg_val_loss)\n        if early_stopper.check(avg_val_loss, model):\n            print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.4f}, Val Loss:{avg_val_loss:.4f},\\nEarly stop in {epoch+1}!!')\n            break\n\n    if (epoch+1) % 1 == 0 or epoch == 0:\n        print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.4f}, Val Loss:{avg_val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-04T06:53:30.932882Z","iopub.execute_input":"2023-09-04T06:53:30.933247Z","iopub.status.idle":"2023-09-04T09:58:53.397002Z","shell.execute_reply.started":"2023-09-04T06:53:30.933213Z","shell.execute_reply":"2023-09-04T09:58:53.395573Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:44<00:00,  8.12s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/100], Train Loss: 0.7525, Val Loss:5.5607\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:38<00:00,  8.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/100], Train Loss: 1.5417, Val Loss:2.2560\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:37<00:00,  8.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/100], Train Loss: 0.8339, Val Loss:1.7435\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:33<00:00,  7.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/100], Train Loss: 1.5045, Val Loss:1.5920\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:37<00:00,  8.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/100], Train Loss: 0.5507, Val Loss:1.4215\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:39<00:00,  8.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/100], Train Loss: 0.1180, Val Loss:0.8902\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:37<00:00,  8.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/100], Train Loss: 0.8293, Val Loss:0.8533\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:33<00:00,  7.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/100], Train Loss: 0.4737, Val Loss:0.4945\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:23<00:00,  7.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/100], Train Loss: 0.1260, Val Loss:0.5046\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:24<00:00,  7.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/100], Train Loss: 0.5086, Val Loss:0.4427\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:24<00:00,  7.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/100], Train Loss: 0.8230, Val Loss:0.4022\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:33<00:00,  7.97s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/100], Train Loss: 0.0676, Val Loss:0.3183\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:38<00:00,  8.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/100], Train Loss: 0.0532, Val Loss:0.2831\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:48<00:00,  8.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/100], Train Loss: 0.0241, Val Loss:0.1282\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:43<00:00,  8.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/100], Train Loss: 0.0685, Val Loss:0.1184\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:40<00:00,  8.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/100], Train Loss: 0.0001, Val Loss:0.2200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 72/72 [09:38<00:00,  8.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/100], Train Loss: 0.0178, Val Loss:0.1377\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 29/72 [04:01<05:58,  8.34s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels, glcm_features \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m     14\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n","Cell \u001b[0;32mIn[31], line 14\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_folder, filename))\n\u001b[0;32m---> 14\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mHaralick_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m image1 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_folder, filename))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n","Cell \u001b[0;32mIn[30], line 2\u001b[0m, in \u001b[0;36mHaralick_features\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mHaralick_features\u001b[39m(image):\n\u001b[0;32m----> 2\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mharalick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m     features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(features)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mahotas/features/texture.py:141\u001b[0m, in \u001b[0;36mharalick\u001b[0;34m(f, ignore_zeros, preserve_haralick_bug, compute_14th_feature, return_mean, return_mean_ptp, use_x_minus_y_variance, distance)\u001b[0m\n\u001b[1;32m    139\u001b[0m         cooccurence(f, \u001b[38;5;28mdir\u001b[39m, cmat, symmetric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, distance\u001b[38;5;241m=\u001b[39mdistance)\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cmat\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mharalick_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_cmatrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mignore_zeros\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_zeros\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpreserve_haralick_bug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_haralick_bug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcompute_14th_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_14th_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreturn_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreturn_mean_ptp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_mean_ptp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_x_minus_y_variance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_x_minus_y_variance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mahotas/features/texture.py:288\u001b[0m, in \u001b[0;36mharalick_features\u001b[0;34m(cmats, ignore_zeros, preserve_haralick_bug, compute_14th_feature, return_mean, return_mean_ptp, use_x_minus_y_variance)\u001b[0m\n\u001b[1;32m    285\u001b[0m px_minus_y\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    286\u001b[0m _texture\u001b[38;5;241m.\u001b[39mcompute_plus_minus(p, px_plus_y, px_minus_y)\n\u001b[0;32m--> 288\u001b[0m feats[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpravel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpravel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m feats[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(k2, px_minus_y)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m sy \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.\u001b[39m:\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":35}]}